{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audioop import mul\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn import preprocessing\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.FloatTensor(10,1).normal_(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_feature\n",
    "vis_emb = np.load(r\"C:\\HolisticVideoUnderstanding\\sampled_test\\applying cream\\0PgfOKP1Ow0_66\\InceptionResnetV2_MaxPooling.npz\")[\"InceptionResnetV2\"]\n",
    "print(vis_emb.shape)\n",
    "\n",
    "# textual_feature\n",
    "txt_emb = np.load(r\"C:\\HolisticVideoUnderstanding\\sampled_test\\applying cream\\0PgfOKP1Ow0_66\\Elmo_Mean.npz\")[\"Elmo\"]\n",
    "print(txt_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Loader\n",
    "class HVUDataset(Dataset):\n",
    "    def __init__(self, n_trial):\n",
    "        self.total = n_trial\n",
    "        \n",
    "    def __len__(self):        \n",
    "        return self.total # 4\n",
    "    \n",
    "    # this function will return one n-way, k-shot set\n",
    "    def __getitem__(self,idx):\n",
    "        \n",
    "        #return torch.randperm(idx)\n",
    "        return \"erol___\"+str(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = HVUDataset(n_trial=30)\n",
    "    \n",
    "train_loader = DataLoader(dataset= ds, batch_size = 10, shuffle=True, num_workers = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for local_batch in train_loader:\n",
    "    print(local_batch)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty = []\n",
    "empty_test = []\n",
    "\n",
    "a = [np.ones(3)*1, np.ones(3)*2, np.ones(3)*3, np.ones(3)*4]\n",
    "b = [np.ones(3)*10, np.ones(3)*20, np.ones(3)*30, np.ones(3)*40]\n",
    "\n",
    "empty.extend(a[:-1])\n",
    "empty_test.extend(a[-1:])\n",
    "\n",
    "empty.extend(b[:-1])\n",
    "empty_test.extend(b[-1:])\n",
    "\n",
    "\n",
    "print(empty)\n",
    "print(empty_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "normalizer = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_f = np.load(\"C:/HolisticVideoUnderstanding/uniform_train/air drumming/qF1dEUpUbNA_80/InceptionResnetV2_MaxPooling.npz\")['InceptionResnetV2']\n",
    "textual_f = np.load(\"C:/HolisticVideoUnderstanding/uniform_train/air drumming/qF1dEUpUbNA_80/Elmo_Mean.npz\")['Elmo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(visual_f), np.min(visual_f), np.max(visual_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_visual_f = normalizer.fit_transform(np.float32(visual_f).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(new_visual_f), np.min(new_visual_f), np.max(new_visual_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(textual_f), np.min(textual_f), np.max(textual_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_textual_f = normalizer.fit_transform(np.float32(textual_f).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(new_textual_f), np.min(new_textual_f), np.max(new_textual_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Distribution Aware Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = [0,0,0,0,0, 1,1,1,1,1,2,2,2,2,2]\n",
    "n_way = 3\n",
    "k_shot = 5\n",
    "\n",
    "# Training Stage\n",
    "mu_1 = torch.normal(10, 1, (5,64))\n",
    "mu_2 = torch.normal(-30, 1, (5,64))\n",
    "mu_3 = torch.normal(170, 1, (5,64))\n",
    "\n",
    "logvar_1 = torch.normal(10, 1, (5,64))\n",
    "logvar_2 = torch.normal(-30, 1, (5,64))\n",
    "logvar_3 = torch.normal(170, 1, (5,64))\n",
    "\n",
    "mus = torch.cat((mu_1, mu_2, mu_3))\n",
    "logvars = torch.cat((logvar_1, logvar_2, logvar_3))\n",
    "\n",
    "# Testing Stage\n",
    "mu_1 = torch.normal(7, 1, (5,64))\n",
    "mu_2 = torch.normal(-20, 1, (5,64))\n",
    "mu_3 = torch.normal(100, 1, (5,64))\n",
    "\n",
    "logvar_1 = torch.normal(7, 1, (5,64))\n",
    "logvar_2 = torch.normal(-20, 1, (5,64))\n",
    "logvar_3 = torch.normal(100, 1, (5,64))\n",
    "\n",
    "test_mus = torch.cat((mu_1, mu_2, mu_3))\n",
    "test_logvars = torch.cat((logvar_1, logvar_2, logvar_3))\n",
    "\n",
    "#mus = torch.rand((15,1,64)).reshape((n_way*k_shot, 64))\n",
    "#logvars = torch.rand((15,1,64)).reshape((n_way*k_shot, 64))\n",
    "#test_mus = torch.rand((15,1,64)).reshape((n_way*k_shot, 64))\n",
    "#test_logvars = torch.rand((15,1,64)).reshape((n_way*k_shot, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mus.shape, logvars.shape, test_mus.shape, test_logvars.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.a) Class mus, logvar construction stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1 / k_shot\n",
    "avg_train_mus = []\n",
    "avg_train_vars = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(mus) - 1, k_shot):\n",
    "    \n",
    "    avg_mu = np.mean(mus[i: i + k_shot].cpu().detach().numpy(), axis=0)\n",
    "    \n",
    "    class_mu = mus[i: i + k_shot].cpu().detach().numpy()\n",
    "    class_variance = logvars[i: i + k_shot].cpu().detach().numpy()\n",
    "        \n",
    "    # sinif ici her bir mu variance al\n",
    "    avg_var = [0.0] * 64\n",
    "    for j in range(len(class_variance)):\n",
    "        \n",
    "        # For the 1st, 2nd, 3rd solutions            \n",
    "        if k_shot != 1:\n",
    "            avg_var += (( 1/k_shot * class_variance[j] ) + (1/k_shot * ((class_mu - avg_mu)**2)))\n",
    "        # If there is only 1 sample, avg_var equals to that sample's var\n",
    "        else:\n",
    "            avg_var += class_variance[j]        \n",
    "            \n",
    "    # class oriented var\n",
    "    avg_train_vars.append(avg_var)\n",
    "    \n",
    "    # class oriented mu\n",
    "    avg_train_mus.append(avg_mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_train_mus[0].shape, avg_train_mus[1].shape, avg_train_mus[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_train_vars[0].shape, avg_train_vars[1].shape, avg_train_vars[2].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) Testing stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(avg_train_mus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "gt_tensor = np.array(gt, dtype=np.int32)\n",
    "gt_tensor = torch.from_numpy(gt_tensor).type(torch.long).reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_labels = []\n",
    "\n",
    "for i in range(len(test_mus)):\n",
    "    test_dist = torch.distributions.Normal(test_mus[i],test_logvars[i])\n",
    "\n",
    "    local_scores = []\n",
    "    for j in range(len(avg_train_mus)):\n",
    "        \n",
    "        # KL divergence\n",
    "        train_dist = torch.distributions.Normal(torch.Tensor(avg_train_mus[j]), torch.Tensor(avg_train_vars[j]))\n",
    "        print(\"Train Dist\")\n",
    "        print(train_dist)\n",
    "        scr = torch.distributions.kl_divergence(test_dist, train_dist).mean()\n",
    "        print(\"Test Dist\")\n",
    "        print(test_dist)\n",
    "        #print(\"{}.th sample, {}.th class, SCORE:{}\".format(i,j,scr))\n",
    "        local_scores.append(scr)\n",
    "        \n",
    "    #print(local_scores)\n",
    "    \n",
    "    local_similarities = torch.from_numpy(np.array([-1 * elem for elem in local_scores])).reshape(1,-1)\n",
    "    \n",
    "    # categorical cross entropy loss    \n",
    "    #print(local_similarities)    \n",
    "    #print(gt_tensor[i])\n",
    "    cc_loss = loss(local_similarities, gt_tensor[i])\n",
    "    \n",
    "    # min of kl divergence for class decision\n",
    "    pred_cls_idx = np.argmin(local_scores)\n",
    "    \n",
    "    print(pred_cls_idx)\n",
    "    print(cc_loss)\n",
    "    print(\"----------\")\n",
    "        \n",
    "    test_pred_labels.append(pred_cls_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = [-0.2025, -1.7995, -3.1739]\n",
    "raw_exp = [np.exp(elem) for elem in raw]\n",
    "probabilities = [elem/np.sum(raw_exp) for elem in raw_exp]\n",
    "\n",
    "probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pred \\t G.T.\")\n",
    "for i in range(len(test_pred_labels)):\n",
    "    print(\"{} \\t {}\".format(test_pred_labels[i], gt[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(gt, test_pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "'''\n",
    "Hypothesis Function - Sigmoid function\n",
    "'''\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    " \n",
    "'''\n",
    "yHat represents the predicted value / probability value calculated as output of hypothesis / sigmoid function\n",
    " \n",
    "y represents the actual label\n",
    "'''\n",
    "def cross_entropy_loss(yHat, y):\n",
    "    if y == 1:\n",
    "      return -np.log(yHat)\n",
    "    else:\n",
    "      return -np.log(1 - yHat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1.0\n",
      "1\n",
      "6.015991412996108e-120\n",
      "0\n",
      "5.703876292213237e-172\n",
      "-----------\n",
      "inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:20: RuntimeWarning: divide by zero encountered in log\n"
     ]
    }
   ],
   "source": [
    "gt = [0,1,0]\n",
    "raw = [-272.06121, -546.5770, -666.3647]\n",
    "exp = []\n",
    "\n",
    "for elem in raw:\n",
    "    exp.append((math.exp(elem)))\n",
    "    \n",
    "sum_exp = 0.0\n",
    "\n",
    "for elem in exp:\n",
    "    sum_exp += elem\n",
    "    \n",
    "\n",
    "probability = []\n",
    "\n",
    "for elem in exp:\n",
    "    probability.append(elem / sum_exp)   \n",
    "\n",
    "sum_loss = 0.0\n",
    "\n",
    "for gt_elem,prob in zip(gt, probability):\n",
    "    print(gt_elem)\n",
    "    print(prob)\n",
    "    sum_loss += cross_entropy_loss(prob, gt_elem)\n",
    "    \n",
    "    \n",
    "print(\"-----------\")\n",
    "print(sum_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13_03_2022_16_05_21\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H_%M_%S\")\n",
    "\n",
    "today = datetime.today()\n",
    "current_day = today.strftime(\"%d_%m_%Y_\")\n",
    "\n",
    "\n",
    "print(current_day+current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13_03_2022\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(d1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 9.7297e-6\n",
    "b = 1.0832e-6\n",
    "c = 5.6362e-9\n",
    "\n",
    "t = a + b + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8993545725714722, 0.10012445121734677, 0.0005209762111809544)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1 = a / t\n",
    "p2 = b / t\n",
    "p3 = c / t\n",
    "\n",
    "p1, p2, p3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bd5a1bf88f2223e086c21887dd6e48bf3dffebb406704354dbccd7318b75d67b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('pytorch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
